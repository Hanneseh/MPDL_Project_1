{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wkMV9Dd6znu",
        "outputId": "585fa6bf-73b0-4e94-c782-5ea82ff8d2ae"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers\n",
        "\n",
        "import datasets\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import math\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models.resnet import ResNet50_Weights\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Pad, ConvertImageDtype, Lambda\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sy8O_avJ6OyQ",
        "outputId": "883f4ef2-c560-4c57-a227-d82b0753419d"
      },
      "outputs": [],
      "source": [
        "################################# load, process and split the data #################################\n",
        "# download the source datasets\n",
        "imagenette_dataset = datasets.load_dataset(\"frgfm/imagenette\", \"full_size\", split=\"train\")\n",
        "diffusion_db_dataset = datasets.load_dataset(\"poloclub/diffusiondb\", \"2m_first_1k\", split=\"train\")\n",
        "\n",
        "# delete uneccessary columns\n",
        "diffuision_columns_to_drop = ['prompt', 'seed', 'step', 'cfg', 'sampler', 'user_name', 'timestamp', 'image_nsfw', 'prompt_nsfw', 'width', 'height']\n",
        "diffusion_db_dataset = diffusion_db_dataset.remove_columns(diffuision_columns_to_drop)\n",
        "imagenette_dataset = imagenette_dataset.remove_columns(['label'])\n",
        "\n",
        "# Add numeric labels to the datasets\n",
        "diffusion_labels = np.zeros(len(diffusion_db_dataset), dtype=np.int64)\n",
        "diffusion_labels += 1\n",
        "diffusion_db_dataset = diffusion_db_dataset.add_column(\"label\", diffusion_labels)\n",
        "\n",
        "imagenette_label = np.zeros(len(imagenette_dataset), dtype=np.int64)\n",
        "imagenette_dataset = imagenette_dataset.add_column(\"label\", imagenette_label)\n",
        "\n",
        "# Create a new features schema with a ClassLabel object\n",
        "class_label = datasets.ClassLabel(names=['fake', 'real'])\n",
        "new_features = datasets.Features(\n",
        "    {\n",
        "        'image': datasets.Image(),\n",
        "        'label': class_label\n",
        "    }\n",
        ")\n",
        "\n",
        "diffusion_db_dataset = diffusion_db_dataset.map(lambda example: {'label': class_label.str2int('fake')}, remove_columns=['label'], features=new_features)\n",
        "imagenette_dataset = imagenette_dataset.map(lambda example: {'label': class_label.str2int('real')}, remove_columns=['label'], features=new_features)\n",
        "\n",
        "# concatenate both datasets to one\n",
        "combined_dataset = datasets.concatenate_datasets([imagenette_dataset, diffusion_db_dataset])\n",
        "\n",
        "# create test, train and validation split and keep class balance\n",
        "# Split the combined dataset into 80% for train and 20% for a temporary dataset\n",
        "train_and_temp_splits = combined_dataset.train_test_split(test_size=0.2, seed=42, stratify_by_column=\"label\")\n",
        "train_dataset = train_and_temp_splits['train']\n",
        "\n",
        "# Split the temporary dataset into 50% test and 50% validation (each will be 7.5% of the original combined dataset)\n",
        "test_and_validation_splits = train_and_temp_splits['test'].train_test_split(test_size=0.5, seed=42, stratify_by_column=\"label\")\n",
        "test_dataset = test_and_validation_splits['test']\n",
        "validation_dataset = test_and_validation_splits['train']\n",
        "\n",
        "# Create a DatasetDict containing the three splits\n",
        "dataset = datasets.DatasetDict({\"train\": train_dataset, \"test\": test_dataset, \"val\": validation_dataset})\n",
        "\n",
        "# print dataset keys and values\n",
        "for ds in dataset:\n",
        "    print(ds, dataset[ds])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "################################# funstions to process an element #################################\n",
        "\n",
        "def get_image_size(combined_dataset):\n",
        "    # Calculate the aspect ratios and sizes of all images in the combined dataset\n",
        "    aspect_ratios = []\n",
        "    sizes = []\n",
        "\n",
        "    for example in tqdm(combined_dataset):\n",
        "        image = example['image']\n",
        "        height, width = image.size\n",
        "        aspect_ratios.append(width / height)\n",
        "        sizes.append((width, height))\n",
        "\n",
        "    # Compute the median aspect ratio and size\n",
        "    median_aspect_ratio = np.median(aspect_ratios)\n",
        "    median_size = np.median(sizes, axis=0)\n",
        "\n",
        "    # Calculate the new dimensions based on the median aspect ratio\n",
        "    target_size = 375  # This is the target size (width or height) you want for your images\n",
        "    new_width = int(target_size * median_aspect_ratio)\n",
        "    new_height = target_size\n",
        "\n",
        "    # Check if the width is greater than the height, and adjust accordingly\n",
        "    if new_width > new_height:\n",
        "        new_width, new_height = new_height, new_width\n",
        "\n",
        "    # Calculate padding\n",
        "    padding_width = (target_size - new_width) // 2\n",
        "    padding_height = (target_size - new_height) // 2\n",
        "    return new_width, new_height, padding_width, padding_height\n",
        "\n",
        "new_width, new_height, padding_width, padding_height = get_image_size(combined_dataset)\n",
        "\n",
        "# Function to convert an image to 3 channels\n",
        "def to_3_channels(image):\n",
        "    if image.shape[0] == 3:\n",
        "        return image\n",
        "    return image.repeat(3, 1, 1)\n",
        "\n",
        "def collate_fn(examples):\n",
        "    images, labels = [], []\n",
        "\n",
        "    image_transform = Compose([\n",
        "        Lambda(lambda x: x.convert('RGB')),  # Convert all images to RGB\n",
        "        Resize((new_height, new_width)),\n",
        "        Pad((padding_width, padding_height, math.ceil(padding_width), math.ceil(padding_height))),\n",
        "        ToTensor(),\n",
        "        Lambda(to_3_channels),  # Ensure all images have 3 channels\n",
        "        ConvertImageDtype(torch.float32)  # Convert image dtype to float32\n",
        "    ])\n",
        "\n",
        "    # Iterate through the examples, apply the image transformation, and append the results\n",
        "    for example in examples:\n",
        "        image = image_transform(example['image'])\n",
        "        label = example['label']\n",
        "        images.append(image)\n",
        "        labels.append(label)\n",
        "\n",
        "    # Stack the images and labels\n",
        "    pixel_values = torch.stack(images)\n",
        "    labels = torch.tensor(labels)\n",
        "\n",
        "    return {\"pixel_values\": pixel_values, \"label\": labels}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "################################# define the model and parameters #################################\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set hyperparameters\n",
        "num_epochs = 1\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "\n",
        "train_dataloader = DataLoader(dataset['train'], collate_fn=collate_fn, batch_size=batch_size)\n",
        "val_dataloader =  DataLoader(dataset['val'], collate_fn=collate_fn, batch_size=batch_size)\n",
        "\n",
        "model = torchvision.models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "tr = model.train()\n",
        "\n",
        "train_accu = []\n",
        "train_losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "################################# train the model #################################\n",
        "\n",
        "def train(epoch,data):\n",
        "    print('\\nEpoch : %d'%epoch)\n",
        "    correct = 0\n",
        "    running_loss=0\n",
        "    total=0\n",
        "    for element in data:\n",
        "        # Move input and label tensors to the device\n",
        "        inputs = element[\"pixel_values\"].to(device)\n",
        "        labels = element[\"label\"].to(device)\n",
        "\n",
        "        # Zero out the optimizer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    train_loss=running_loss/len(data)\n",
        "    accu=100.*correct/total\n",
        "\n",
        "    train_accu.append(accu)\n",
        "    train_losses.append(train_loss)\n",
        "    print('Train Loss: %.3f | Accuracy: %.3f'%(train_loss,accu))\n",
        "\n",
        "eval_losses=[]\n",
        "eval_accu=[]\n",
        "\n",
        "def val(data):\n",
        "    correct = 0\n",
        "    running_loss=0\n",
        "    total=0\n",
        "    with torch.no_grad():\n",
        "      for element in data:\n",
        "          # Move input and label tensors to the device\n",
        "          inputs = element[\"pixel_values\"].to(device)\n",
        "          labels = element[\"label\"].to(device)\n",
        "\n",
        "        \n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "          running_loss += loss.item()\n",
        "\n",
        "          _, predicted = outputs.max(1)\n",
        "          total += labels.size(0)\n",
        "          correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    val_loss=running_loss/len(data)\n",
        "    accu=100.*correct/total\n",
        "\n",
        "    eval_accu.append(accu)\n",
        "    eval_losses.append(val_loss)\n",
        "    print('Val Loss: %.3f | Accuracy: %.3f'%(val_loss,accu))\n",
        "\n",
        "for epoch in range(1,num_epochs+1):\n",
        "  train(epoch,train_dataloader)\n",
        "  val(val_dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "################################# plot trainin stats #################################\n",
        "\n",
        "plt.plot(train_accu,'-o')\n",
        "plt.plot(eval_accu,'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['Train','Valid'])\n",
        "plt.title('Train vs Valid Accuracy')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(train_losses,'-o')\n",
        "plt.plot(eval_losses,'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('losses')\n",
        "plt.legend(['Train','Valid'])\n",
        "plt.title('Train vs Valid Losses')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the model\n",
        "PATH = './resnet50_hannes_v1.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "################################# test the model #################################\n",
        "\n",
        "model.eval()\n",
        "image = Image.open('dow.jpg')\n",
        "  \n",
        "# Define a transform to convert PIL \n",
        "# image to a Torch tensor\n",
        "transform = Compose([\n",
        "    ToTensor()\n",
        "])\n",
        "  \n",
        "# transform = transforms.PILToTensor()\n",
        "# Convert the PIL image to Torch tensor\n",
        "\n",
        "img_tensor = transform(image)\n",
        "\n",
        "inputs = img_tensor.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(inputs.unsqueeze(0))\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    \n",
        "    print(predicted)\n",
        "\n",
        "dataset[\"train\"].features[\"label\"].names[predicted]\n",
        "\n",
        "model.eval()\n",
        "image = Image.open('flob.jpg')\n",
        "  \n",
        "# Define a transform to convert PIL \n",
        "# image to a Torch tensor\n",
        "transform = Compose([\n",
        "    ToTensor()\n",
        "])\n",
        "  \n",
        "# transform = transforms.PILToTensor()\n",
        "# Convert the PIL image to Torch tensor\n",
        "\n",
        "img_tensor = transform(image)\n",
        "\n",
        "inputs = img_tensor.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(inputs.unsqueeze(0))\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    \n",
        "    print(predicted)\n",
        "\n",
        "dataset[\"train\"].features[\"label\"].names[predicted]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
