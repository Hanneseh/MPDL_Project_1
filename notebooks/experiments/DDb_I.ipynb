{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import Compose, ColorJitter, ToTensor, Resize\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset,ClassLabel,Value,concatenate_datasets,Features,Array2D\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset diffusiondb (/home/hakim/.cache/huggingface/datasets/poloclub___diffusiondb/2m_first_10k/0.9.1/547894e3a57aa647ead68c9faf148324098f47f2bc1ab6705d670721de9d89d1)\n",
      "100%|██████████| 1/1 [00:00<00:00, 220.03it/s]\n",
      "Found cached dataset diffusiondb (/home/hakim/.cache/huggingface/datasets/poloclub___diffusiondb/2m_random_5k/0.9.1/547894e3a57aa647ead68c9faf148324098f47f2bc1ab6705d670721de9d89d1)\n",
      "100%|██████████| 1/1 [00:00<00:00, 673.68it/s]\n",
      "Found cached dataset imagenette (/home/hakim/.cache/huggingface/datasets/frgfm___imagenette/160px/1.0.0/38929285b8abcae5c1305418e9d8fea5dd6b189bbbd22caba5f5537c7fa0f01f)\n",
      "100%|██████████| 2/2 [00:00<00:00, 714.41it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_diffusion_train = load_dataset(\"poloclub/diffusiondb\",\"2m_first_10k\")\n",
    "dataset_diffusion_test = load_dataset(\"poloclub/diffusiondb\",\"2m_random_5k\")\n",
    "dataset_imagenette = load_dataset(\"frgfm/imagenette\",\"160px\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db = dataset_diffusion_train[\"train\"].select(range(9469))\n",
    "val_db = dataset_diffusion_test[\"train\"].select(range(1962))\n",
    "test_db = dataset_diffusion_test[\"train\"].select(range(1962,3925))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_I = dataset_imagenette[\"train\"]\n",
    "val_I = dataset_imagenette[\"validation\"].select(range(1962))\n",
    "test_I = dataset_imagenette[\"validation\"].select(range(1962,3925)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = len(train_db) * [0]\n",
    "train_db = train_db.add_column(\"label\",label_column)\n",
    "label_column = len(val_db) * [0]\n",
    "val_db = val_db.add_column(\"label\",label_column)\n",
    "label_column = len(test_db) * [0]\n",
    "test_db = test_db.add_column(\"label\",label_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_col = ['prompt','seed', 'step','cfg','sampler','width','height','user_name','timestamp','image_nsfw', 'prompt_nsfw']\n",
    "train_db = train_db.remove_columns(del_col)\n",
    "val_db = val_db.remove_columns(del_col)\n",
    "test_db = test_db.remove_columns(del_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_I = train_I.remove_columns(\"label\")\n",
    "val_I = val_I.remove_columns(\"label\")\n",
    "test_I = test_I.remove_columns(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = len(train_I) * [1]\n",
    "train_I = train_I.add_column(\"label\",label_column)\n",
    "label_column = len(val_I) * [1]\n",
    "val_I = val_I.add_column(\"label\",label_column)\n",
    "label_column = len(test_I) * [1]\n",
    "test_I = test_I.add_column(\"label\",label_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = concatenate_datasets([train_I,train_db])\n",
    "val = concatenate_datasets([val_I,val_db])\n",
    "test = concatenate_datasets([test_I,test_db])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitter = Compose(\n",
    "    [ ToTensor(),Resize((160,160))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(examples):\n",
    "\n",
    "    examples[\"pixel_values\"] = [jitter(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train.with_transform(transforms)\n",
    "val_dataset = val.with_transform(transforms)\n",
    "test_dataset = train.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set hyperparameters\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "\n",
    "    images = []\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for example in examples:\n",
    "\n",
    "        images.append((example[\"pixel_values\"]))\n",
    "\n",
    "        labels.append(example[\"label\"])\n",
    "\n",
    "        \n",
    "\n",
    "    pixel_values = torch.stack(images)\n",
    "\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return {\"pixel_values\": pixel_values, \"label\": labels}\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=batch_size)\n",
    "val_dataloader =  DataLoader(val_dataset, collate_fn=collate_fn, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, collate_fn=collate_fn, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accu = []\n",
    "train_losses = []\n",
    "\n",
    "def train(epoch,data):\n",
    "    print('\\nEpoch : %d'%epoch)\n",
    "    correct = 0\n",
    "    running_loss=0\n",
    "    total=0\n",
    "    for element in data:\n",
    "        # Move input and label tensors to the device\n",
    "        inputs = element[\"pixel_values\"].to(device)\n",
    "        labels = element[\"label\"].to(device)\n",
    "\n",
    "        # Zero out the optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss=running_loss/len(data)\n",
    "    accu=100.*correct/total\n",
    "\n",
    "    train_accu.append(accu)\n",
    "    train_losses.append(train_loss)\n",
    "    print('Train Loss: %.3f | Accuracy: %.3f'%(train_loss,accu))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_losses=[]\n",
    "eval_accu=[]\n",
    "\n",
    "def val(data):\n",
    "    correct = 0\n",
    "    running_loss=0\n",
    "    total=0\n",
    "    with torch.no_grad():\n",
    "      for element in data:\n",
    "          # Move input and label tensors to the device\n",
    "          inputs = element[\"pixel_values\"].to(device)\n",
    "          labels = element[\"label\"].to(device)\n",
    "\n",
    "        \n",
    "          outputs = model(inputs)\n",
    "          loss = criterion(outputs, labels)\n",
    "          running_loss += loss.item()\n",
    "\n",
    "          _, predicted = outputs.max(1)\n",
    "          total += labels.size(0)\n",
    "          correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_loss=running_loss/len(data)\n",
    "    accu=100.*correct/total\n",
    "\n",
    "    eval_accu.append(accu)\n",
    "    eval_losses.append(val_loss)\n",
    "    print('Val Loss: %.3f | Accuracy: %.3f'%(val_loss,accu))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hakim/anaconda3/envs/py38/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.201 | Accuracy: 97.661\n",
      "Val Loss: 6.928 | Accuracy: 50.000\n",
      "\n",
      "Epoch : 2\n",
      "Train Loss: 0.240 | Accuracy: 96.420\n",
      "Val Loss: 6.439 | Accuracy: 50.000\n",
      "\n",
      "Epoch : 3\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "for epoch in range(1,epochs+1):\n",
    "  train(epoch,train_dataloader)\n",
    "  val(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
